---
layout: post
title:  "Don't Learn to Code, Learn to Program (But Come Back in 10 Years)"
categories: tech
tags: engineering programming
---

I want to discuss why programming and coding are not the same, and why I can no
longer support the Learn to Code movement. I can no longer in good conscience
tell my friends to learn coding. Although I'm passionate about it, it's too
arcane and insular for mainstream practice. Come back when programming gets it
together.

It pains me to say so, because there is a creator’s high in producing complex,
invisible machines made of little more than thought. I want to share it.
Programming is a worthy notion **to extend human capability**, by offloading
humanly-infeasible work onto a machine. The promise of an **amplified knowledge
worker**.

Yet we barely have programming today. All we seem to have is coding. It is less
of an extension of human capability than it is a brittle hack.

> If builders built buildings the way programmers wrote programs, then the
> first woodpecker that came along would destroy civilization.
>
> -- Gerald M. Weinberg, The Psychology of Computer Programming (1971)

Coding is a pure text, watered-down implementation of programming. Nowhere near
an intuitive, hands on experience like painting or crafting. To oversimplify,
the inclusion or omission of a text character **in ever-changing code, not
necessarily your own code** can make all the difference if your company’s
website stays up today, or the button on your nuclear missile is secure. This
is unintuitive and error prone. It’s mile-high Jenga and the rest of the world
piled on their own custom blocks while you weren't looking.

To ameliorate this, there are human engineering techniques, but they treat the
symptoms, not the root cause. [Coding's accessibility to humans has stagnated
in the past few decades][The Future of Programming]. I don’t mean accessibility
to just outsiders, because it is a large mess that hardcore coders themselves
can hardly keep up with. I take a step back from mashing together 2 things that
were never meant to work together, for the 100th time today, and wonder how
anything ever gets done. What cool thing was I trying to build before I got
sidetracked with this tedious mashing?

What progress has been made? Programming ideas from several decades ago are
only coming to mainstream, begrudging acceptance now. For example, functional
programming wasn't just a weird academic toy from the mid 1900s; it has a
certain provable foundation, and evident speed & maintenance advantages. A
relief from the unintuitive, unscalable, unfounded buzz-concepts like OOP,
markup, APIs, or the triumvirate of HTML/CSS/JS. As if these technologes are
the best we can do. Yet these snake oils reign supreme.

Speaking of these technologies we seem to be stuck with, much of the discussion
is heated debates about today's text-based languages: what order of operations
do you prefer, number of special keywords, maximum textual expressiveness in as
little space as possible. All mere incremental improvements to the programming
ecosystem. Not to mention that highly local edits to these projects could still
trigger a Woodpecker Level Event in them and others. And boy do these coders
like to debate. They take pride in mastering the most **arcane** of interfaces,
tweaks, and underlying low-level architectures. More than any layman knowledge
worker would ever want to know.

Take this proud tweet for example. Someone figured out the correct cocktail
recipe to be a web developer.

<blockquote class="twitter-tweet"><p>Everything you need to win at web development: <a href="https://twitter.com/nodejs">@nodejs</a> <a href="https://twitter.com/MongoDB">@MongoDB</a> <a href="https://twitter.com/gruntjs">@gruntjs</a> <a href="https://twitter.com/bower">@bower</a> <a href="https://twitter.com/angularjs">@angularjs</a> <a href="https://twitter.com/jquery">@jquery</a> <a href="https://twitter.com/twbootstrap">@twbootstrap</a> <a href="https://twitter.com/fontawesome">@fontawesome</a> <a href="https://twitter.com/livereload">@livereload</a></p>&mdash; Justin Klemm (@justinklemm) <a href="https://twitter.com/justinklemm/statuses/396324049754009600">November 1, 2013</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

How is anybody supposed to discover & master 9 disparate technologies? Bake
them off against all their alternatives?

Can I just make a website?

Meanwhile there are [few][Wired - Light Table] [souls][NoFlo] looking to evolve
text code into something more humanly **intuitive**. Say for example something
you can touch, don’t have to read, or that tells you what the computer is
thinking, so you don’t have to think like a computer yourself.

What are coders doing then? They can’t constantly debate or think about how to
take revolutionary leaps forward; nothing might get done today. Besides, what
would it mean to give up the tools they invested so much in? So I think they
suck it up, remember their pride in knowing how to do what they do, and they
write another line of code. As if another line of code were a net win for the
world.

Jeff Atwood’s [Please Don’t Learn To Code][Please Don't Learn To Code] is the
most pragmatic anti-Learn to Code movement piece.

> You should be learning to write as little code as possible. Ideally none …
> Software developers tend to be software addicts who think their job is to
> write code. But it’s not. Their job is to solve problems.

My amendment to Atwood’s article is, the coding culture he describes is majorly
responsible for programming’s stagnation. And because the best programming
teachers come from the programming industry, we get an **insular** cycle of
this-is-the-way-I-learned-to-program-so-will-you. So the apprentice furiously
practices their rite of passage, writing another line of code. In text.

My friends occasionally ask me if they should follow one of these Learn to Code
websites, either curious about what I do or because they hear it’s a lucrative
career. If you understand it and you have fun, I won't stop you. I don’t feel
as strongly as Atwood, who says you’ll be a net loss to the world.

But I can’t recommend coding casually, because it is an arcane and insular art.
It is ridiculously hostile to the layman. You have to be a masochist more than
anything, first to get good and then to endure. Coding is not the great promise
of programming, extending human capability. I love what computers have done for
me and what they can do for people. But come back in 10 years when the promise
of programming hopefully bears fruit, when programming becomes accessible, when
coding is a thing of the past.

<blockquote class="twitter-tweet"><p>&quot;Programming for the masses&quot; won&#39;t be successful if people have to change to suit programming. Programming should change to suit the people.</p>&mdash; Chris Granger (@ibdknox) <a href="https://twitter.com/ibdknox/statuses/396372478982750208">November 1, 2013</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet"><p>To be clear, I don&#39;t want to bring &quot;code&quot; or &quot;writing software&quot; to the masses at all. That&#39;s scary, arcane nonsense that I want to disappear</p>&mdash; Chris Granger (@ibdknox) <a href="https://twitter.com/ibdknox/statuses/396378696086339584">November 1, 2013</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

[The Future of Programming]: http://vimeo.com/71278954
[Wired - Light Table]: http://www.wired.com/wiredenterprise/2014/01/light-table/
[NoFlo]: http://noflojs.org/
[Please Don't Learn To Code]: http://www.codinghorror.com/blog/2012/05/please-dont-learn-to-code.html
